{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train_data = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test_data = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "  \n",
    "                       ]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test_data, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The torch.nn import gives us access to some helpful neural network things, such as various neural network layer types (things like regular fully-connected layers, convolutional layers (for imagery), recurrent layers...etc).\n",
    "\n",
    "- The torch.nn.functional area specifically gives us access to some handy functions that we might not want to write ourselves. We will be using the relu or \"rectified linear\" activation function for our neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #Typically, when you inherit from a parent class, that init method doesn't actually get run. This is how we can run that init method of the parent class\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features= 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1) #dim=1 defines across which axis should the sum be 1. Since we have batch_size =10 and no. of labels also 10 (0 to 9). SO we would apply softmax across axis=1\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our first layer takes in 28x28, because our images are 28x28 images of hand-drawn digits. A basic neural network is going to expect to have a flattened array, so not a 28x28, but instead a 1x784.\n",
    "\n",
    "Then this outputs 64 connections. This means the next layer, fc2 takes in 64 (the next layer is always going to accept however many connections the previous layer outputs). From here, this layer ouputs 64, then fc3 just does the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cretae a random image\n",
    "X = torch.randn((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten before feeding it to NN\n",
    "X = X.view(-1,28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should understand the 28*28 part, but why the leading -1?\n",
    "\n",
    "Any input and output to our neural network is expected to be a group feature sets.\n",
    "\n",
    "Even if you intend to just pass 1 set of features, you still have to pass it as a \"list\" of features.\n",
    "\n",
    "In our case, we really just want a 1x784, and we could say that, but you will more often is -1 used in these shapings. Why? -1 suggests \"any size\". So it could be 1, 12, 92, 15295...etc. It's a handy way for that bit to be variable. In this case, the variable part is how many \"samples\" we'll pass through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3274, -2.4268, -2.3198, -2.2927, -2.2916, -2.2836, -2.2275, -2.2084,\n",
       "         -2.2677, -2.4013]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) #net.parameters() refers to everything that is trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1138, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0010, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0308, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS): # 3 full passes over the data\n",
    "    for data in trainset:  # `data` is a batch of featuresets and labels\n",
    "        X, y = data  # X is the batch of features, y is the batch of targets. So X[0] is the first image\n",
    "        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
    "        output = net(X.view(-1,28*28))  # pass in the reshaped batch (recall they are 28x28 atm)\n",
    "        loss = F.nll_loss(output, y)  # since our label is a scaler value i.e 0 to 9 and not one-hot vector hence we used nll_loss. If labels were one-hot vector use MSE.\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
    "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
    "    print(loss)  # print loss.  it only output the loss of the last batch of the data for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  The output of the network is log of probability (due to F.log_softmax(x, dim=1)) so it is all negative and it doesn't sum up to 1, it sums up to a negative value. The training works out because we use the 'F.nll_loss(output,y)' which is the negative log likelihood loss and it requires the inputs exactly we have provided them, where the output is log probabilities of each class and the labels are just a class index. (0,1,2,3...)\n",
    "\n",
    "- Once we pass data through our neural network, getting an output, we can compare that output to the desired output. With this, we can compute the gradients for each parameter, which our optimizer (Adam, SGD...etc) uses as information for updating weights.\n",
    "\n",
    "This is why it's important to do a net.zero_grad() for every step, otherwise these gradients will add up for every pass, and then we'll be re-optimizing for previous gradients that we already optimized for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.965\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad(): #Don't upadte the weights while passing this time while passing the data through NN\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        #print(output)\n",
    "        for idx, i in enumerate(output):\n",
    "            #print(torch.argmax(i), y[idx])\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"model.train()\" and \"model.eval()\" activates and deactivates Dropout and BatchNorm, so it is quite important. \"with torch.no_grad()\" only deactivates gradient calculations, but doesn't turn off Dropout and BatchNorm. Your model accuracy will therefore be lower if you don't use model.eval() when evaluating the model.\n",
    "- If you want to evaluate your model on the test set, you should use \"model.eval()\" to get the real accuracy and also use \"with torch.no_grad()\" at the same time to avoid calculating the gradients, which saves time (During evaluation you are not optimizing your model, so you don't need to calculate gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[0].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breakdown of above code\n",
    "a_featureset = X[0]\n",
    "reshaped_for_network = a_featureset.view(-1,784) # 784 b/c 28*28 image resolution.\n",
    "output = net(reshaped_for_network) #output will be a list of network predictions.\n",
    "first_pred = output[0]\n",
    "print(first_pred)\n",
    "biggest_index = torch.argmax(first_pred)\n",
    "print(biggest_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a20f9aec864725aaab65e871419d99cbd7cf3d5ab0d2e7243b7ef9750fa61ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('pixsy_work': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
